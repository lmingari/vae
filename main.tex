% vim: set textwidth=0:
%
\documentclass[acp]{copernicus}
\usepackage{xspace}
\usepackage{bm}

\newcommand*{\eg}{e.g.\@\xspace}
\newcommand*{\ie}{i.e.\@\xspace}
\graphicspath{{figs/}}

\begin{document}

\title{A generative AI model for volcanic ash dispersion}

\Author[1][lmingari@csic.es]{Leonardo}{Mingari} %% correspondence author
\Author[1]{Arnau}{Folch}
\Author[1]{Heribert}{Pascual}
\Author[2]{Manuel}{Titos}

\affil[1]{Geosciences Barcelona (GEO3BCN-CSIC), Barcelona, Spain}
\affil[2]{University of Granada, Granada, Spain}

\runningtitle{AI model for volcanic ash dispersion}

\runningauthor{L. Mingari}

\received{}
\pubdiscuss{} %% only important for two-stage journals
\revised{}
\accepted{}
\published{}

\firstpage{1}

\maketitle

\begin{abstract}
Ensemble modelling of volcanic plumes transport is a popular approach for dealing with uncertainties in eruption source parameters, meteorological driver or systematic errors in numerical models. Most of the ensemble modelling applications, such as quantification of forecast uncertainties, probabilistic volcanic hazard assessment or data assimilation, require large ensembles to mitigate sampling errors and properly capture probability distributions. 
%
However, Volcanic Ash Transport and Dispersal (VATD) models are computationally demanding, making the simulation of large ensembles prohibitively expensive even for High-Performance Computing (HPC) clusters. Consequently, operational forecasting is typically restricted to small ensembles in order to align model requirements with the available computational resources.
%
In contrast, generative AI models are capable of generating large volumes of physically consistent data in a very short time frame at very low computational cost. Some examples of physics-informed generative models preserving the underlying governing physical laws include Physics-Informed Neural Networks (PINNs), Diffusion Models with physical constraints, and GANs augmented with conservation laws.
%
In this work, a simple Variational Autoencoder (VAE) is trained using an ensemble of numerical simulations generated by a physical model. A small training ensemble is used to replicate the typical configuration found in operational modelling frameworks. We show that this Machine Learning (ML) strategy can generate a large ensembles of physically grounded data of volcanic ash concentration almost instantaneously, allowing a small ensemble of VATD model simulations to be expanded and enriched. The statistical properties of the VAE-generated ensemble are characterised in detail and the VAE performance is evaluated against a reference dataset composed of 2048 numerical simulations.
%
Finally, we discuss possible future applications of this work, including latent space data assimilation via deep learning.
\end{abstract}

\introduction 
%
% General overview
%
Real-time operational forecasts of volcanic ash plumes typically rely on numerical simulations based on physical models to make predictions of ash concentrations or potential aviation hazards. Volcanic ash transport and dispersal (VATD) models describe multiple process, such as plume dynamics, atmospheric transport, strength and distribution of emissions, particle sedimentation or deposition. However, the complexity of the physical phenomena, multiplicity of scales involved and the uncertainty in the quantification of the eruptive source term or the meteorological fields, make the modelling of volcanic plumes challenging and forecasts based on numerical simulations prone to errors.
%
% Model uncertainties
%
In particular, the so-called Eruption Source Parameters (ESP), such as those required to estimate the column top height, mass eruption rate, particle size distribution or vertical mass distribution, are recognised to be first-order contributors to model errors \citep{costa2016,poulidis2021}. The determination of the ESP is one of the major challenges in volcanic plume modelling due to limited real-time observations, complex eruptive dynamics and rapid variability in emission patterns.
%
% Ensemble modelling
%
Ensemble modelling is a popular approach to deal with uncertainties associated to model parameters, spatial distribution and time dependency of eruptive emissions or meteorological data. In recent years, the increasing availability of high-performance computing (HPC) resources has significantly facilitated the use of ensemble modelling and this approach has been successfully applied across a wide range of scientific and engineering domains.
%
% Ensemble applications: error estimation, hazards studies, data assimilation
%
With the help of ensembles, it has been possible to characterise and quantify forecast uncertainties due to poorly constrained input parameters, model errors or the meteorological driver, allowing for the enrichment of traditional forecast products based on deterministic models.
%
Volcanic hazard assessment is of paramount importance to support risk-based decision-making and emergency management during volcanic crisis. Quantitative volcanic hazard assessments, in particular, require estimating probabilities of a location being affected by a volcanic eruption~\citep{marzocchi2006} and can benefit from ensembles with a large numbers of numerical simulations~\citep{titos2022}.
%
In addition, model errors can be reduced incorporating observations into numerical models by means of ensemble-based data assimilation techniques~\citep{kalnay2003}. Data assimilation (DA) techniques have been widely used to study and forecast geophysical systems and have been applied in a variety of research and operational settings~\citep{carrasi2018}. Similarly, several ensemble-based methods have been applied to the modelling of volcanic plumes~\citep{folch2023}, including ensemble Kalman filter methods~\citep{fu2016,fu2017,osores2020,pardini2020,mingari2022} and ensemble particle filter methods~\citep{zidikheri2021,zidikheri2021b}.

%
% Ensembles and computational costs
%
Most of geophysical models are computationally demanding, making the simulation of large ensembles prohibitively expensive even for High-Performance Computing (HPC) clusters, especially in the context of real-time operational services. In order to accommodate the available computing resources, ensembles are usually limited to a few dozen of simulations in operational forecasting, introducing important sampling errors. 
%
% Generative AI models
%
Generative AI models can generate large ensembles with thousands of samples almost instantaneously at very low computational cost. To this purpose, AI models must be trained to generate results in agreement with the underlying physical laws governing the atmospheric dispersion and transport of volcanic ash. The Variarional Autoencoder or VAE~\citep{kingma2013} represent one of the most prominent example of generative model. A VAE a probabilistic mapping from data to a continuous latent space, enabling the reconstruction and synthesis of new samples by sampling from this learned representation.

%
% Latent space data assimilation
%
Recently, several studies have demonstrated the benefits of using a low dimensional latent space for data assimilation purposes~\citep{pasmans2025}. \citet{chen2025} trained a VAE using sea ice reanalysis and conducted assimilation experiments by means of a particle filter technique. The Latent Space Particle Filter (LSPF) approach takes advantage of a variational autoencoder to extract low-dimensional representations of sea ice physical fields, enabling a large number of low-dimensional samples to be generated. In this way, model errors can be significantly reduced by assimilating satellite observations of sea ice concentration and thickness.
%
Alternatively, \citet{pasmans2025} proposed a variant of the ensemble transform Kalman filter~\citep{bishop2001} in which the analysis update is applied in the latent space of a VAE. In addition, they also showed that introducing a second latent space for the observational innovations the performance can be improved when non-Gaussian observations errors are present.

%
% Objectives
%
In this work, we train a convolutional variational autoencoder in order to generate thousands of samples of atmospheric volcanic ash concentration. In particular, the training is performed using two-dimensional data of mass loading or vertical column mass per unit area of airborne ash. The training, validation and test datasets are generated through the numerical model FALL3D~\citep{folch2020,prata2021}. By perturbing eruption source parameters and meteorological fields, FALL3D can produce an ensemble of simulations in a very efficient way running a parallel job with multiple GPUs~\citep{folch2022}. We show that we can expand and enrich a small ensemble simulated via a physical model using a Machine Learning (ML) strategy. This paper focuses on characterising the statistical properties of the VAE-generated ensemble and evaluating the VAE performance, while a study about latent space data assimilation via deep learning is postponed for a second article.
%
% Paper structure
%
The manuscript is organised as follows. Section~\ref{sec:datasets} describes the numerical model, the model configuration and the datasets used for training and performance evaluation. The neural network architecture is described in Sect.~\ref{sec:vae} along with a brief overview of the theoretical background behind variational autoencoders. Results are presented in Sect.~\ref{sec:results} and performance is evaluated in terms of different metrics. Finally, conclusions are drawn in Sect.~\ref{sec:conclusions}, where some implications of this work and possible future applications are discussed.

\section{Physical model and datasets} \label{sec:datasets}
A physical model is used to generate the datasets required for training and evaluation of the variational autoencoder. Specifically, a volcanic ash transport and dispersion (VATD) model was used to simulate the atmospheric transport of ash from a volcanic source. The ensembles of numerical simulations were performed using the latest version release of FALL3D (v9.1), an open source code. FALL3D is an Eulerian model for atmospheric passive transport and deposition of different atmospheric species (\ie, volcanic ash and gases, mineral dust, and radionuclides) and is based on the so-called Advection-Diffusion-Sedimentation (ADS) equation \citep{folch2020,prata2021}. The numerical model is GPU-accelerated using the OpenACC parallel programming model and has been designed to support increasingly larger scientific workloads in preparation for the transition to extreme-scale computing systems \citep{folch2023b}. It has been optimised for the main pre-exascale supercomputer in Europe: LUMI (Finland), Leonardo (Italy) and MareNostrum 5 (Spain).

Ensemble modelling allows one to characterise and quantify model uncertainties due to poorly constrained input parameters and errors in the model physics parameterisations or the underlying model-driving meteorological data. In addition, the ability to generate ensemble runs makes it possible to improve forecasts by incorporating observations using different ensemble-based data assimilation techniques. FALL3D can be run ensembles of simulation in a efficient way through a single parallel task \citep{folch2022}. However, executing a large ensemble of numerical simulations is a high-demanding computational task, even for HPC clusters, and the resources requirement often exceeds the operational capacity of forecasting centres.

\subsection{Ensemble of numerical simulations}
The EU-MODEX Tenerife 2025, organised under the European Civil Protection Mechanism, represented the first large-scale eruption drill in Spain to practice the response to a potential volcanic eruption in Tenerife (Canary Islands) and test evacuation, social assistance and communication during a volcanic crisis. On 26 September 2025, we provided real-time ash-dispersion forecasts to support emergency decision-making using the FALL3D model. These forecasts will be used in this work. The configuration of the FALL3D model is summarised in Table~\ref{tab:model}. We've performed a 24-h forecast assuming continuous emission starting at 09:00UTC. A three-dimensional computational domain with a horizontal resolution of 0.1\degree and $120 \times 100 \times 25$ grid points was defined. The dispersal model was driven by the Global Forecast System (GFS) weather forecast from the National Centers for Environmental Prediction (NCEP).

\begin{table}[h]
    \caption{FALL3D model configuration.}
    \label{tab:model}
    \begin{tabular}{lc}
    \tophline
    Parameter & Value \\ \middlehline
    Start time & 26 September 09:00UTC \\
    Run time & 24~h \\
    Resolution & 0.1\degree $\times$ 0.1\degree \\
    Number of grid points & 120$\times$100$\times$25 \\
    Species & 3 tephra bins \\
    Grain Size Distribution  & bi-Gaussian \\
    Emission source & Suzuki source \citep{pfeiffer2005} \\
    Mass emission rate & Estimated~\citep{woodhouse2016} \\
    Ensemble size & 64,256 and 2048 \\
    \bottomhline
    \end{tabular}
\end{table}

The ensembles were generated by perturbing the Eruption Source Parameters (ESP) and the horizontal wind components using the Latin Hypercube Sampling \citep[LHS,][]{mckay1979} to efficiently sample the parameter space. Table~\ref{tab:ensemble} lists the reference value and sampling uncertainty range for each perturbed parameter. A constant eruptive column top height was assumed for each ensemble member sampled around the central value $H=9~\text{km avl}$ (above vent level). In addition, the wind components were perturbed assuming a sampling range of $25\%$. The eruptive column height and the wind are widely reconsigned as the most sensitive parameters for volcanic ash transport models (add REF).

\begin{table}[h]
  \caption{Ensemble configuration. The perturbed model parameters are: eruption column height ($H$), mass eruptive rate ($MER$), Suzuki parameter $A_s$ and the wind horizontal components.}
    \label{tab:ensemble}
    \begin{tabular}{lccc}
    \tophline
    Parameter        & Reference value        & Distribution & Sampling range  \\ \middlehline
    $H$              & 15~km avl$^\dag$       & Uniform      & $\pm$30\%       \\
    $MER$            & $\sim$1.6E6~kg/s$^\ddag$ & Uniform      & $\pm$25\%       \\
    $A_s$            & 4                      & Uniform      & 1               \\
    U wind           & GFS$^\S$               & Uniform      & $\pm$25\%       \\
    V wind           & GFS$^\S$               & Uniform      & $\pm$25\%       \\ \bottomhline
    \end{tabular}
    \belowtable{%
        $^\dag$ Height given in km above the vent level; \\
        $^\ddag$ Estimated from $H$ according to~\citet{woodhouse2016}; \\
        $^\S$ Global Forecast System (GFS) weather forecast from NCEP.
    }
\end{table}

A total of three numerical simulations were conducted in order to generate a training dataset (simulation A), a validation dataset (simulation B) and test dataset (simulation C). As summarized in Table~\ref{tab:runs}, simulation A produces an ensemble with 256 members, which is used to train the VAE. The ensemble consisting of 64 members generated by simulation B is used during training to monitor performance and tuning hyperparameters. The 2048-member ensemble is used at the end to evaluate the final model and provides a fair (unbiased) report of model performance.

\begin{table}[h]
  \caption{Dataset built from ensemble of numerical simulations.}
  \label{tab:runs}
\begin{tabular}{lll}
  \tophline
  Simulation & Ensemble size & Dataset    \\ \middlehline
  A          & 256           & Training   \\
  B          & 64            & Validation \\
  C          & 2048          & Test       \\ \bottomhline
\end{tabular}
\end{table}

\section{Variational Autoencoder} \label{sec:vae}
A Variational Autoencoder (VAE) is a type of generative machine learning model based on a neural network architecture capable of learning an efficient, probabilistic encoding of data into a latent space of low dimensionality~\citep{kingma2013}. By sampling vectors $\vec{z}$ from the learned distribution, a decoder can generate new and realistic data samples, even when the network was trained to encode complex and high-dimensional input data. The scheme in Fig.~\ref{fig:vae} shows a typical architecture of a VAE composed of convolutional layers. The encoder maps inputs $\vec{x}$ to the parameters of a probability distribution (\eg, a Gaussian with parameters $\vec{\mu}$ and $\vec{\sigma}$) over the latent space, from which a latent vector $\bm{z}$ is sampled. This vector is passed to the decoder, which aims to reconstruct the original input. 

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{f01}
  \caption{The basic scheme of a variational autoencoder composed of
  two-dimensional convolutional layers. The encoder compresses an input
  $\vec{x}$ into the low-dimensional latent space. The decoder attempts
  to reconstruct the original data from a vector $\vec{z}$ sampled from the
  latent space. }
  \label{fig:vae}
\end{figure*}

The VAE training is guided by a loss function balancing the fidelity of the data reconstruction against a term that regularizes the latent distributions toward a simple prior $p(\vec{z})$:

\begin{equation}
  \mathcal{L}_{VAE}(\theta,\phi;\vec{x}) = -\mathbb{E}_{q_{\phi}(\vec{z}\vert\vec{x})}
  \left[ \log p_{\theta}(\vec{x} \vert \vec{z}) \right] + 
  D_{KL} \left( q_{\phi}(\vec{z}\vert\vec{x}) \Vert p(\vec{z}) \right)
  \label{eq:vae_loss}
\end{equation}
where $q_{\phi}(\vec{z}\vert\vec{x})$ is the encoder approximate posterior and $p_{\theta}(\vec{x} \vert \vec{z})$ the decoder likelihood.

The first term (reconstruction loss) encourages reconstructions to be accurate and, in this work, is assumed to be the mean squared error (MSE) between the input ($\vec{x}$) and its reconstruction (\vec{x}'):
\begin{equation}
  \mathcal{L}_R = \dfrac{1}{N} \sum_{i=1}^N (x_i - x'_i)^2
\end{equation}

The second term in (\ref{eq:vae_loss}) is defined in terms of the Kullback-Leibler (KL) divergence, $D_{KL}$, and measures the difference between the approximate posterior $q_{\phi}(\vec{z} \vert \vec{x})$ and the prior distribution $p(\vec{z})$. In practice, the KL term forces $q_{\phi}$ to be close to the prior $p(\vec{z})$. Traditionally, the standard VAE assumes a diagonal multivariate normal distribution for the approximate posterior, $q_{\phi}(\vec{z} \vert \vec{x}) = \mathcal {N}(\vec{\mu}, \vec{\sigma^2} \mathbb{I} )$, and a standard normal distribution for the prior, $p(\vec{z}) = \mathcal {N}(\vec{0},\mathbb{I})$, leading to:

\begin{equation}
  \mathcal{L}_{KL} = -\dfrac{1}{2} \sum_{i=1}^k \left[1+\log \sigma_i^2 - \mu_i^2 - \sigma_i^2 \right]
\end{equation}
being $\vec{\mu} = (\mu_1,\dots,\mu_k)^{\intercal}$ the vector of means, $\vec{\sigma^2} = (\sigma_1^2,\dots,\sigma_k^2)^{\intercal}$ the vector of variances and $k$ the dimension of $\vec{z}$.

Finally, the $\beta$-VAE is a modification of the standard VAE that uses a hyperparameter, $\beta$, to weight the KL divergence term in the loss function:
\begin{equation}
  \mathcal{L}_{VAE} = \mathcal{L}_R + \beta \, \mathcal{L}_{KL}
\end{equation}
This is the expression of the loss function used in this work.

\subsection{Training}
A Variational Autoencoder (VAE) was implemented in the PyTorch framework~\citep{paszke2019} using an architecture based on two-dimensional convolutional layers. The VAE configuration is defined by list of hyperparameters reported in Table~\ref{tab:hyperparameters}. The neural network was trained in batches using the Adam optimizer for 400 epochs. The reconstruction loss and KL divergence (averaged over the batches) are reported for each epoch in Fig.~\ref{fig:loss} for both the training and validation datasets in order to prevent overfitting. The training (validation) dataset is composed by a 256(64)-member ensemble of modelled volcanic ash column mass on a two-dimensional grid of size $120 \times 100$. As outlined in the scheme of Fig.~\ref{fig:vae}, the encoder network maps the input data $\vec{x} \in \mathbb{R}^{120\times100}$ through three successive convolutional layers, followed by a flattening operation, to two distinct latent space parameter vectors: $\vec{\mu}$ (mean) and $\vec{\sigma}^2$ (log-variance). The decoder then reconstructs the input $\vec{x'}$ from the sampled latent vector $\vec{z} \in \mathbb{R}^{16}$ using a fully connected layer followed by three transposed convolutional layers (or deconvolutional layers), restoring the spatial dimensions, \ie $\vec{x'} \in \mathbb{R}^{120\times100}$.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\textwidth]{loss}
  \caption{%
    Training history for the variational autoencoder (VAE) through 400 epochs. %
    The reconstruction loss ($\mathcal{L}_R$) and KL divergence ($\mathcal{L}_{KL}$) %
    are shown separately for the training and validation datasets for each epochs. %
    The total loss function is given by $\mathcal{L}_R + \beta \, \mathcal{L}_{KL}$. 
    In this configuration we used a latent dimension of 16 and a $\beta=6$.
    }
  \label{fig:loss}
\end{figure}

Figure~\ref{fig:latent_space}a shows the training and test datasets encoded in latent space (just two arbitrary components of the latent vector are shown, \ie $z_5$ vs $z_4$). As expected by construction, the components of the latent vector are distributed according independent standard normal distributions: $z_i \sim \mathbb{N}(0,1)$. A very low correlation between every pair $(z_i,z_j)$ is confirmed by the correlation matrix (Fig.~\ref{fig:latent_space}b). Notice that the samples from the test dataset are filling the same area as the samples used for training the network. All these remarks as a whole indicate that the network was correctly trained and works as expected.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\textwidth]{latent_space}
  \caption{%
    Latent space representation of the training and test datasets. %
    As an example, the encoding in the latent plane $(z_5,z_4)$ 
    is shown in a scatter plot (a). %
    The independence between the distributions of the latent vector 
    components can be confirmed from the correlation matrix (b). 
    }
  \label{fig:latent_space}
\end{figure}

\begin{table}[t]
  \caption{Hyperparameters used for the VAE training.} \label{tab:hyperparameters}
  \begin{tabular}{lccc}
    \tophline
    Hyperparameter       & Value \\ \middlehline
    Latent dimension     & 16 \\
    $\beta^\dag$         & 6 \\
    Data size            & 120$\times$100 \\
    Number of epochs     & 400 \\
    Batch size           & 16 \\ 
    Convolutional layers & 6 \\
    Output channels      & 16, 32 and 64 \\
    Kernel Size          & 3$\times$3 \\
    Stride               & 1$\times$1 \\
    Activation function  & ReLU \\
    Learning rate        & 1E-3 \\
    Optimizer            & Adam \\
    Normalization        & Percentile-based scaling \\
    \bottomhline
  \end{tabular}
  \belowtable{%
    $^\dag$ Scaling factor for the KL divergence term in the $\beta$-VAE formulation, defined in Eq.~(\ref{eq:vae_loss}).
  }
\end{table}

\subsection{Evaluation metrics} \label{sec:metrics}
Since the VAE was trained using a physical model, in the best case scenario, the samples generated by the VAE, $\left\{ \vec{x_i} \right\}$, should ideally resemble the distribution of a large ensemble produced by the same physical model, $\left\{ \vec{\hat{x}_i} \right\}$. Consequently, probabilistic distributions or sample statistics, such as the sample mean or sample standard deviation, are compared against the reference ensemble $\left\{ \vec{\hat{x}_i} \right\}$, defined by the test dataset consisting of 2048 simulations. In other words, we train a generative AI model using a 256-member ensemble and the performance is evaluated against a 2048-member ensemble.

A few performance metrics are proposed to quantify the ability of the generative AI model to produce realistic ash concentration outputs. For example, the Root-Mean-Square Error (RMSE)
\begin{equation}
  \text{RMSE} = \sqrt {\frac{1}{p}\sum^p_{j=1} (y_j-\hat{y}_j)^2}
  \label{eq:rmse}
\end{equation}
can be evaluated in terms of the sample means, \ie $\vec{y} = 1/N \sum \vec{x_i}$ and $\vec{\hat{y}} = 1/\hat{N} \sum \vec{\hat{x}_i}$. The Mean Bias Error (MBE) is computed from:

\begin{equation}
  \text{MBE} =\frac{1}{p}\sum^p_{j=1} y_j-\hat{y}_j
  \label{eq:mbe}
\end{equation}

The mean absolute percentage error (MAPE), provides a relative measure of the prediction accuracy:

\begin{equation}
  \text{MAPE} = 100 \times \sum^p_{j=1} \left\vert \dfrac{y_j - \hat{y}_j}{\hat{y}_j} \right\vert
  \label{eq:mape}
\end{equation}

The KL divergence is a measure of the difference between two probability distributions. Specifically, for two discrete distributions $P = \{p_i\}$ and $Q=\{q_i\}$, the KL divergence can be interpreted as the expected logarithmic difference between the probability distributions:
\begin{equation}
  D_{KL}(P \Vert Q) = \sum p_i \log \dfrac{p_i}{q_i}
  \label{eq:kl}
\end{equation}
The discrete distributions are estimated from histograms in this work.

The two-dimensional isotropic power spectrum, $P(k)$, is computed in order to assess the ability of the VAE to generate realistic samples over different spatial scales, in consistency with physical models. It's defined as the radially averaged 2D Power Spectrum Density (PSD) and is a function of the wavenumber magnitude, $k$:
\begin{equation}
  P(k) = \dfrac{1}{N_k} \sum PSD(k_x,k_y)
  \label{eq:psd}
\end{equation}
where $k=\sqrt{k_x^2 + k_y^2}$ is the magnitude of the wavevector, $PSD(k_x,k_y)$ is the two-dimensional Power Spectral Density obtained from a Fast Fourier Transform (FFT), and $N_k$ is the number of data points within a annular ring around $k$ in the Fourier space.

\section{Results and discussion} \label{sec:results}
Once the VAE neural network has been trained with numerical simulations based on a physical model, it's possible to generate new physics-grounded samples at very low computational cost that are consistent with the underlying physical laws governing atmospheric dispersion. Figure~\ref{fig:vae_ensemble} shows a VAE-generated ensemble consisting of 12 samples. The samples represent two-dimensional ash column mass, \ie the vertically integrated ash mass per unit area. The variability in the ensemble is a consequence of the different eruptive source parameters (\eg, eruptive column height) and meteorological fields used to simulate the training dataset. For examples, samples in Fig.~\ref{fig:vae_ensemble}c,d correspond to cases with high eruptive column heights and strong emission, whereas other samples, such as~Fig.~\ref{fig:vae_ensemble}h, are associated to lower column heights and weak emission.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{vae_ensemble}
  \caption{%
    The neural network was trained to generate samples %
    of two-dimensional ash column mass, \ie the total %
    mass of volcanic ash in a vertical column.
    In this example, a VAE-generated ensemble composed of 12 samples is shown. %
    }
  \label{fig:vae_ensemble}
\end{figure*}

\subsection{Sensitivity study}
We trained multiple VAEs changing the latent space dimension and $\beta$
(see~\ref{eq:vae_loss}), while fixing the remaining hyperparameters as specified in Table~\ref{tab:hyperparameters}. The performance was evaluated in term of the ensemble mean RMSE (see~\ref{eq:rmse}). To this purpose, 2048-member ensembles were generated and the RMSE was computed for each ensemble. Since the VAE is a probabilistic model, each sampled ensemble will yield different results. The boxplot in Fig.~\ref{fig:boxplot} reflects this variability considering 20 ensembles generated using the same hyperparameters. In all cases, the network was trained for 400 epochs, except for the configurations with latent dimension of 4 and 2, where early stopping was required to avoid overfitting. According to the trend suggested by Fig.~\ref{fig:boxplot}, the best performance is achieved for latent dimensions above 6 and $\beta\gtrsim 6$. Next, we present results using a latent dimension of 16 and $\beta=6$, as indicated in Table~\ref{tab:hyperparameters}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\textwidth]{boxplot}
  \caption{VAE performance for different hyperparameters in term of the ensemble mean RMSE.}
  \label{fig:boxplot}
\end{figure}

\subsection{Ensemble mean and spread}
The ensemble mean and spread (standard deviation) are shown in Fig.~\ref{fig:mean_spread} for the training dataset (a,d), a 5000-member ensemble generated by the VAE (b,e) and the test dataset (c,f). The statistics computed for the training dataset and the VAE are similar to those computed for the test dataset, although some differences can be identified in Fig.~\ref{fig:mean_spread}e for the VAE ensemble spread. A summary of some metrics is shown in Table~\ref{tab:metrics}. The VAE metrics are averaged over 20 ensembles generated to account for the variability introduced by the stochastic sampling. The metrics are evaluated taking the test dataset as reference, as mentioned in Sect.~\ref{sec:metrics}. The VAE errors measured in terms of the RMSE are twice as high as those evaluated from the training dataset. In relative terms, this represent a difference of 1.78\% (training) and $3.4(6)\%$ (VAE) according to the MAPE evaluated from the ensemble mean. In order to avoid divisions by zero and numerical instabilities, the MAPE was computed where the reference is above $5~g~m^{-2}$ (see Fig.~\ref{fig:mean_spread}c). We can conclude that VAE is properly capturing the first and second moments of the distribution, although the training estimations for the ensemble mean and spread are still slightly better. Next, we'll show VAE have a better performance for statistics more sensitive to sampling errors.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{mean_spread}
  \caption{%
    Ensemble mean (a,b,c) and ensemble spread (d,e,f) computed from %
    the training dataset (left column), ensemble VAE with 5000 samples %
    (central column) and the test dataset (right column).
    }
  \label{fig:mean_spread}
\end{figure*}

\begin{table}[t]
  \caption{
    A summary of some performance metrics evaluated taking the test dataset as reference.
    The VAE metrics were computed over 20 ensembles generated to account for stochastic variability.
    The average is reported along with an uncertainty range given by the standard deviation.
  }
  \label{tab:metrics}
  \begin{tabular}{lccc}
    \tophline
    Metric    & Variable  & Training dataset  & VAE (2048 samples) \\ \middlehline
    RMSE      & Mean      & 0.108             & $0.20  \pm 0.03$   \\
    MBE       & Mean      & 0.001             & $-0.04 \pm 0.02$   \\
    MAPE (\%) & Mean      & 1.784             & $3.4   \pm 0.6$    \\ \middlehline
    RMSE      & Spread    & 0.255             & $0.56  \pm 0.04$   \\
    MBE       & Spread    & -0.004            & $-0.17 \pm 0.01$   \\
    \bottomhline
  \end{tabular}
  \belowtable{ In units of $g m^{-2}$, except MAPE.  }
\end{table}

\subsection{Probability distributions}
A proper assessment of natural hazard involves a good estimation of probabilities. For example, it may be desirable to estimate the exceedance probability, \ie the probability that a specific hazard's value will be exceeded within a given timeframe. For this reason, it is essential to have an accurate description of probability distributions. However, empirical distribution estimation based on small ensembles can be challenging due to sampling errors. Figure~\ref{fig:exceedance} shows a few exceedance probabilities for different thresholds. Again, the probability maps reveals that the VAE is able to generate realistic samples with the correct probability distribution.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{exceedance_probability}
  \caption{Exceedance probabilities for three thresholds: 8,14 and 20~$g~m^{-2}$. Left column shows the results for the training dataset, central column corresponds to a VAE ensemble of 5000 samples and the right column shows the results for the test dataset.}
  \label{fig:exceedance}
\end{figure*}

In order to examine if the VAE ensemble provides a better approximation to the actual distribution, the KL divergence, $D_{KL}$, is computed to quantify the difference between probability distributions~(see \ref{eq:kl}). The discrete probability distribution corresponding to the test dataset is taken as the reference distribution. The results in Fig.~\ref{fig:kl_divergence} indicate that VAE yields much lower $D_{KL}$, meaning that VAE is providing a better approximation of the actual distribution.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\textwidth]{kl_divergence}
  \caption{%
    Kullback Leibler (KL) divergence computed from the training dataset (a) %
    and the VAE-generated samples (b). From these results, it is possible %
    conclude that the actual probability distribution is better %
    approximated by the VAE ensemble.
    }
  \label{fig:kl_divergence}
\end{figure}

To explore this point further, a few histograms for different locations are shown in Fig.~\ref{fig:histograms} comparing results for the training dataset (left column) and a VAE ensemble with 5000 samples (right column). As expected, the distribution for the training dataset with a small ensemble size (256) results in a very noisy distribution, while the VAE distribution provides a good approximation of the test dataset, even for multimodal distributions (\eg, Fig.~\ref{fig:histograms}b).

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{histograms}
  \caption{%
    Histograms for the training datasets (256 samples, left column) and 
    a VAE ensemble (2048 samples, right column) at the different locations.
    The results are compared against the test dataset (5000 samples),
    considered as a reference (yellow shaded area).
    }
  \label{fig:histograms}
\end{figure*}

\subsection{Isotropic power spectrum}
The isotropic power spectrum, \ie the radially averaged two-dimensional Power Spectral Density (PSD), was computed for each sample for the ensemble using (\ref{eq:psd}). The ensemble average is presented in Fig.~\ref{fig:psd} for samples generated by the VAE and for the test dataset. In both cases the ensemble size is 2048. Instead of the classical energy cascade spectrum according to $\propto k^{-5/3}$, the dispersal model exhibits an exponential decay as consequence of the diffusion processes modelled by FALL3D (solid black line). The generative model reproduces large scales but fails at scales smaller, as shown in Fig.~\ref{fig:psd} (blue circles). In this case, the isotropic power spectrum deviates from exponential decay for wavenumbers above $0.25~\Delta x^{-1}$, approximately, where $\Delta x$ is the grid size. In other words, the variational autoencoder is unable to reproduce the variability over spatial scales below $\sim 4~\Delta x$.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\textwidth]{psd}
  \caption{The isotropic power spectrum, \ie radially averaged two-dimensional Power Spectral Density, reveals inconsistencies in the spatial structure of the samples generated by the variational autoencoder. The generative model reproduces large scales but fails at scales smaller since the AI model is introducing artificial noise for large k. Since the wavenumber is expressed in terms of the grid size, $\Delta x$, we can conclude that the VAE is unable to reproduce the variability over spatial scales below $\sim 4~\Delta x$, approximately. }
  \label{fig:psd}
\end{figure}

To illustrate this point, Fig.~\ref{fig:bias} shows a map of the bias computed in terms of the ensemble mean, where a noisy pattern with small intensity and high-frequency structure can be identified in the VAE results.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\textwidth]{bias}
  \caption{%
    Bias computed from the ensemble mean for the training dataset (a) and the VAE samples (b).
    The generative AI model produces a noisy pattern with small intensity and high-frequency.
    }
  \label{fig:bias}
\end{figure}

\conclusions \label{sec:conclusions}
%
%Short summary
This paper has explored the properties of a generative AI model for volcanic ash transport based on a Variational Autoencoder (VAE), a type of machine learning model capable of learning a probabilistic representation of data in a low dimensional latent space and generate new data, similar to that simulated by a physical model.

\codeavailability{
  FALL3D v9.1 is available under the version 3
  of the GNU General Public License (GPL) at
  \url{https://gitlab.com/fall3d-suite/fall3d/} and 
  \url{https://doi.org/10.5281/zenodo.14198155}
  \citep{folch2020,prata2021}.
  %
}

\authorcontribution{
  Conceptualisation, L.M., M.T.; %
  Methodology, L.M.; %
  Software, L.M., A.F., H.P.; %
  Resources, L.M.; %
  Writing---original draft, L.M.; %
  Writing---review and editing, L.M., A.F., H.P., M.T.; %
  Visualisation, L.M.; %
  Supervision, A.F.; %
  Funding Acquisition, A.F.
  %
  All authors have read and approved the final version of the manuscript.
}

\competinginterests{The authors declare that they have no conflict of interest.} 

\begin{acknowledgements}
  This work has been partially funded by the EuroHPC
  Center of Excellence for Exascale in Solid Earth - 
  Second phase (ChEESE-2P) under the Grant Agreement No. 101093038.
\end{acknowledgements}

%% REFERENCES
\bibliographystyle{copernicus}
\bibliography{references.bib}

\end{document}
